import re, sys, glob, os, csv, pprint
import pandas as pd
from RNAja.global_variable import *
import RNAja
from pathlib import Path
from snakemake import WorkflowError
#from snakemake import load_configfile
from os import listdir
import getpass

###############################################################################
# --- Importing Configuration Files --- #

#pprint.pprint(workflow.__dict__)
if len(workflow.overwrite_configfiles) == 0:
    logger.info("You need to use --configfile option to snakemake command line")
    raise ValueError("You have to use --configfile option to snakemake command line")
else:
    path_config = workflow.overwrite_configfiles[0]

#configfile: 'config/config.yaml'
cluster_config: "config/cluster_config_slurm.yaml"

###############################################################################

# helper function
def by_cond(cond, yes, no, cond_ext = '', no_ext = ''): # it's working but needs to be improved ...
    if not cond_ext:
        cond_ext = not cond
    if cond:
        return yes
    elif cond_ext:
        return no
    else:
        return no_ext

## cluster variables

#user = getpass.getuser()

#nasID = config['NASID']
#HOST_PREFIX = by_cond(nasID, user + '@' + nasID + ':', '')

##############################

# check configfile:
# existence of dir and validity of suffix


# parse config file :
out_dir = Path(config["DATA"]["directories"]["out_dir"]).resolve().as_posix()
log_dir = Path(config["DATA"]["directories"]["out_dir"]+"/LOGS/").resolve().as_posix()
annotation_path = Path(config["DATA"]["files"]["annotation"]).resolve().as_posix()
reference_path = Path(config["DATA"]["files"]["reference"]).resolve().as_posix()
samplefile = Path(config["DATA"]["files"]["sample_info"]).resolve().as_posix()
name_hisat_index = "rice"
name_hisat_index_virus = "RYMV"

# to lunch separator
sep="#"

#############################################
# use threads define in cluster_config rule or rule default or default in snakefile
#############################################

def get_threads(rule, default):
    """
    use threads define in cluster_config rule or rule default or default in snakefile
    give threads or 'cpus-per-task from cluster_config rule : threads to SGE and cpus-per-task to SLURM
    """
    #cluster_config = load_configfile(cluster_config)
    if rule in cluster_config and 'threads' in cluster_config[rule]:
        return int(cluster_config[rule]['threads'])
    elif rule in cluster_config and 'cpus-per-task' in cluster_config[rule]:
        return int(cluster_config[rule]['cpus-per-task'])
    elif '__default__' in cluster_config and 'cpus-per-task' in cluster_config['__default__']:
        return int(cluster_config['__default__']['cpus-per-task'])
    elif '__default__' in cluster_config and 'threads' in cluster_config['__default__']:
        return int(cluster_config['__default__']['threads'])
    return default

#*###############################################################################
def final_return(wildcards):
    dico_final = {
                     #"multiqc_fastqc" : expand(f"{out_dir}/5_MULTIQC/multiqc.html"),
                     #"fastqc" : expand(f"{out_dir}/1_QC/fastqc/{{fastq}}_raw_fastqc.html", fastq = SAMPLE_NAME),
                     "baminfo" : f"{out_dir}/2_mapping/bamfile_info.txt",
                     "STRINGTIE" : expand(f'{out_dir}/3_count/STRINGTIE/{{fastq}}.gtf', fastq = SAMPLE_NAME),
                     "STRINGTIE_list" : f'{out_dir}/3_count/STRINGTIE/HISAT_Stringtie_list.txt',
                     #"gffcompare_stat" : f'{out_dir}/3_count/STRINGTIE/merged.stats',
                     "gcsv_hisat" : f'{out_dir}/3_count/STRINGTIE/HISAT_gene_count_matrix.csv',
                     "tcsv_hisat" : f'{out_dir}/3_count/STRINGTIE/HISAT_transcript_count_matrix.csv',
                     "RNA_diff_exp" : f"{out_dir}/4_DE_analysis/diane_Counts.csv"
                     }
    return dico_final

#*###############################################################################

def unique(list1):
    # intilize a null list
    unique_list = []
    for x in list1:
        if x not in unique_list:
            unique_list.append(x)
    return(unique_list)

def checkIfDuplicates_1(listOfElems):
    ''' Check if given list contains any duplicates '''
    if len(listOfElems) == len(set(listOfElems)):
        return False
    else:
        return True

samples = {}
header = ""
with open(samplefile, 'r') as f:
    spamreader = csv.reader(f, delimiter = ",")
    for line in spamreader:
        header = line
        break
    for i in range(len(header)-1):
        with open(samplefile, 'r') as f:
            spamreader = csv.reader(f, delimiter = ",")
            x = 0
            for line in spamreader:
                if line != header :
                    samples[header[i].lower(),x] = line[i]
                    x = x + 1

# on récupère la liste des traitements et des fastq_name en wildcard
treatments = []
sample_name = []
for key in samples:
    if key[0] == "treatment":
        treatments.append(samples[key])
    if key[0] == "samplename":
        sample_name.append(samples[key])
TREATMENT = unique(list(treatments))

SAMPLE_NAME = unique(list(sample_name))

################################

# Récuperation des fastq_path
def get_fastq(wildcards):
    for key in samples:
        if key[0] == "samplename" and samples[key] == wildcards.fastq:
            row = key[1]
            for key1 in samples:
                if key1[0] == "filename":
                    if row == key1[1]:
                        return f"{samples[key1]}"

##############################
# --- Main Build Rules --- #
rule final:
    """
    construct a table of all resume files
    """
    input:
        unpack(final_return)


# ------------- ----------------------------------------- 1 QC:

rule run_Fastqc:
    """
        QC of fastq files on raw fastq files
    """
    threads: get_threads("run_Fastqc", 1)
    input:
        fastq = get_fastq
    output:
        html_fastqc = f"{out_dir}/1_QC/fastqc/{{fastq}}_raw_fastqc.html"
    log:
        error = f"{log_dir}/run_Fastqc/{{fastq}}.e",
        output = f"{log_dir}/run_Fastqc/{{fastq}}.o"
    singularity:
        config["SINGULARITY"]["MAIN"]
    shell:
         """
         fastqc -o {out_dir}/1_QC/fastqc -t {threads} {input.fastq}
         infilename=$(basename {input.fastq})
         fastqcOutFilePath="{out_dir}/1_QC/fastqc/${{infilename%%.*}}"_fastqc.html
         finalOutFilePath="{output.html_fastqc}"
         ## RUN ONLY IF SOURCE AND DEST ARE DIFFERENT PATHS
         [[ "$fastqcOutFilePath" == "$finalOutFilePath" ]] || mv "$fastqcOutFilePath" "$finalOutFilePath"
         """

# --------------------- 1 MAPPING

rule hisat2_index:
    """                                                                                                                                                                                       
    Make index with HISAT2 for 3D7                                                                                                                                                 
    """
    threads: get_threads('bwa_index', 1)
    input:
        reference = reference_path,
    output:
        index = f"{reference_path}{name_hisat_index}",
    message:
        """                                                                                                                                                                                   
        Execute {rule}                                                                                                                                                                    
        input:                                                                                                                                                                            
            reference : {input.reference}                                                                                                                                                       
        output:                                                                                                                                                                           
            index: {output.index}                                                                                                                     
        """
    log:
        error = f"{log_dir}/{name_hisat_index}_HISAT-INDEX.e",
        output = f"{log_dir}/{name_hisat_index}_HISAT-INDEX.o",
    singularity:
        config["SINGULARITY"]["MAIN"]
    shell:
        """
        ln -s {input.reference} {output.index} 1>{log.output} 2>{log.error}
        hisat2-build {input.reference} {output.index} --quiet
        """

# Mapping with HISAT2 on Pf 3D7 (max 2 min par échantillon, total 10 min)
rule hisat2_map:
    """
    Map reads on the genome with HISAT2 on single end
    """
    threads: 4
    input:
        reference = rules.hisat2_index.output.index,
        r1 = get_fastq,
    output:
        summary = f"{out_dir}/2_mapping/{{fastq}}_HISAT_summary.txt",
        bam = f"{out_dir}/2_mapping/{{fastq}}_HISAT.bam",
    message:
        """                                                                                                                                                                            
        Execute {rule}                                                                                                                                                                   
        input:                                                                                                                                                                            
            reference : {input.reference}
            R1 : {input.r1}                                                                                                                                                   
        output:                                                                                                                                                                           
            bam: {output.bam}                                                                                                 
        """
    singularity:
        config["SINGULARITY"]["MAIN"]
    shell:
        """
        hisat2 --dta -p {threads} -x {input.reference} -U {input.r1} --summary-file {output.summary} | samtools view -S -b > {output.bam} 
        """

#Sort the BAM files with samtools
rule bam_sort:
    """
    Sort the BAM files     
    """
    threads: 4
    input:
        hisat=rules.hisat2_map.output.bam
    output:
        out_hisat=f"{out_dir}/2_mapping/{{fastq}}_HISAT_sort.bam",
        mapped_hisat=f"{out_dir}/2_mapping/{{fastq}}_HISAT_sort_mapped.bam",
        unmapped_hisat=f"{out_dir}/2_mapping/{{fastq}}_HISAT_sort_unmapped.bam",
        bai=f"{out_dir}/2_mapping/{{fastq}}_HISAT_sort.bam.bai",
        bai_mapped=f"{out_dir}/2_mapping/{{fastq}}_HISAT_sort_mapped.bam.bai"
    message:
        """
        Execute {rule}
        input:
            hisat : {input.hisat}
        output:
            bam: {output.out_hisat}
            mapped_bam: {output.mapped_hisat}
        """
    singularity:
        config["SINGULARITY"]["MAIN"]
    shell:
        """
        samtools sort  {input.hisat} > {output.out_hisat}
        samtools index {output.out_hisat}
        samtools view -b -F 4 {output.out_hisat} > {output.mapped_hisat}
        samtools view -b -f 4 {output.out_hisat} > {output.unmapped_hisat}
        samtools index {output.mapped_hisat}
        """


rule generate_bamfile_info:
    """
        generate_bamfile_info + bamlist for each treatment (to be use for samtools merge by treatment)
    """
    threads: get_threads('generate_bamfile_info', 1)
    input:
        bam_files = expand(rules.bam_sort.output.mapped_hisat, fastq = SAMPLE_NAME),
        samplefile = samplefile
    params:
        outdir = f"{out_dir}/2_mapping/"
    output:
        out_file = f"{out_dir}/2_mapping/bamfile_info.txt"
    script:
        f"{RNAja.RNAJA_SCRIPTS}/write_bamfile_info.py"


rule samtools_stats:
    """
        make stats on mappings
    """
    threads: get_threads('samtools_stats', 1)
    input:
            sorted_bam_file = rules.bam_sort.output.out_hisat,
            sorted_bam_index = rules.bam_sort.output.bai
    output:
            bamstats = f"{out_dir}/2_mapping/{{fastq}}_HISAT_sort.bam.bamStats.txt",
            idxstats = f"{out_dir}/2_mapping/{{fastq}}_HISAT_sort.bam.idxstats.log",
            flagstat = f"{out_dir}/2_mapping/{{fastq}}_HISAT_sort.bam.flagstat.log"
    log:
            error = f"{log_dir}/samtools_stats/{{fastq}}.e",
            output = f"{log_dir}/samtools_stats/{{fastq}}.o"
    message:
            f"""
            {sep*108}
            Execute {{rule}} for
                Input:
                    - sorted_bam_file : {{input.sorted_bam_file}}
                Output:
                    - bam stats : {{output.bamstats}}
                Others
                    - Threads : {{threads}}
                    - LOG error: {{log.error}}
                    - LOG output: {{log.output}}
            {sep*108}"""
    singularity:
        config["SINGULARITY"]["MAIN"]
    shell:
            """
                samtools stats {input.sorted_bam_file} > {input.sorted_bam_file}.bamStats.txt 2>>{log.error}
                samtools idxstats --threads {threads} {input.sorted_bam_file} > {input.sorted_bam_file}.idxstats.log 2>>{log.error}
                samtools flagstat --threads {threads} {input.sorted_bam_file} > {input.sorted_bam_file}.flagstat.log 2>>{log.error}
            """

rule multiqc:
    """
        multiqc on outdir directory
    """
    threads: get_threads("multiqc", 1)
    input:
        expand(f"{out_dir}/1_QC/fastqc/{{fastq}}_raw_fastqc.html", fastq = SAMPLE_NAME),
        expand(f"{out_dir}/2_mapping/{{fastq}}_HISAT_sort.bam.bamStats.txt", fastq = SAMPLE_NAME),
        expand(f"{out_dir}/2_mapping/{{fastq}}_HISAT_sort.bam.idxstats.log", fastq = SAMPLE_NAME),
        expand(f"{out_dir}/2_mapping/{{fastq}}_HISAT_sort.bam.flagstat.log", fastq = SAMPLE_NAME),
    output:
        f"{out_dir}/5_MULTIQC/multiqc.html"
    log:
        f"{log_dir}/multiqc/multiqc.log"
    wrapper:
        "v1.3.2/bio/multiqc"


################ 3 comptage
rule stringtie_discovery :
    threads: 4
    input:
        bam_index = rules.bam_sort.output.bai_mapped,
        bam_hisat=rules.bam_sort.output.mapped_hisat,
        gtf=f"{annotation_path}"
    output:
        hisat_gtf= f'{out_dir}/3_count/STRINGTIE/{{fastq}}_DIS.gtf',
        hisat_tsv=f'{out_dir}/3_count/STRINGTIE/{{fastq}}_DIS.tsv'
    message:
        """
        Execute {rule}
        """
    singularity:
        config["SINGULARITY"]["MAIN"]
    shell:
        """
        stringtie -p 8 -B -G {input.gtf} -o {output.hisat_gtf} -A {output.hisat_tsv} {input.bam_hisat}
        """

# Create a list of GTF created by Stringtie
rule stringtie_gtf_list_discovery:
    threads : 1
    input:
        list_gtf_hisat = expand(f'{out_dir}/3_count/STRINGTIE/{{fastq}}_DIS.gtf', fastq = SAMPLE_NAME),
    output :
        gtf_hisat = f'{out_dir}/3_count/STRINGTIE/HISAT_gtf_list_DIS.txt',
    shell:
        """
        find {input.list_gtf_hisat} >> {output.gtf_hisat}
        """

# Merge stringtie GTF
rule merge_stringtie_gtf_discovery:
    threads: 4
    input:
        gtf_ref = annotation_path,
        list_gtf_hisat = rules.stringtie_gtf_list_discovery.output.gtf_hisat,
    output:
        gtf_merged = f'{out_dir}/3_count/STRINGTIE/stringtie_merged_DIS.gtf',
    message:
        """
        Execute {rule}
        """
    singularity:
        config["SINGULARITY"]["MAIN"]
    shell:
        """
        stringtie --merge -p 4 -G {input.gtf_ref} -o {output.gtf_merged} {input.list_gtf_hisat}
        """

#Create the count table from the BAM files with stringtie
rule stringtie :
    """
    Create the count table with Stringtie
    """
    threads: 4
    input:
         bam_index = rules.bam_sort.output.bai_mapped,
         bam_hisat = rules.bam_sort.output.mapped_hisat,
         gtf = f"{rules.merge_stringtie_gtf_discovery.output.gtf_merged}" if config["PARAMS"]["STRINGTIE"]["discovery_mode"] else f'{annotation_path}'
    output:
         hisat_gtf= f'{out_dir}/3_count/STRINGTIE/{{fastq}}.gtf',
         hisat_tsv = f'{out_dir}/3_count/STRINGTIE/{{fastq}}.tsv'
    message:
        """
        Execute {rule}
        """
    singularity:
        config["SINGULARITY"]["MAIN"]
    shell:
        """
        stringtie -p 8 -e -B -G {input.gtf} -o {output.hisat_gtf} -A {output.hisat_tsv} {input.bam_hisat}
        """


# Create a list of GTF created by Stringtie
rule stringtie_gtf_list:
    threads : 1
    input:
        list_gtf_hisat = expand(f'{out_dir}/3_count/STRINGTIE/{{fastq}}.gtf', fastq = SAMPLE_NAME),
    output :
        gtf_hisat = f'{out_dir}/3_count/STRINGTIE/HISAT_gtf_list.txt',
    shell:
        """
        find {input.list_gtf_hisat} >> {output.gtf_hisat}
        """

# Merge stringtie GTF
rule merge_stringtie_gtf:
    threads: 4
    input:
        gtf_ref = annotation_path,
        list_gtf_hisat = rules.stringtie_gtf_list.output.gtf_hisat,
    output:
        gtf_merged = f'{out_dir}/3_count/STRINGTIE/stringtie_merged.gtf',
    message:
        """
        Execute {rule}
        """
    singularity:
        config["SINGULARITY"]["MAIN"]
    shell:
        """
        stringtie --merge -p 4 -G {input.gtf_ref} -o {output.gtf_merged} {input.list_gtf_hisat}
        """

# Create a list with the ID sample and the path of the stringtie GTF
rule list_for_prepDE:
    input: rules.stringtie_gtf_list.output.gtf_hisat
    output:
        hisat_list= f'{out_dir}/3_count/STRINGTIE/HISAT_Stringtie_list.txt'
    run:
        hisat_list_name_reads= open(output.hisat_list, "w")
        hisat_gtf = open(rules.stringtie_gtf_list.output.gtf_hisat, "r")
        for i in range(len(SAMPLE_NAME)):
            name=SAMPLE_NAME[i]
            hisat_list_name_reads.write(name + " " + hisat_gtf.readline())


# Convert the stringtie GTF to a count table
rule prepDE_stringtie_table:
    threads : 1
    input :
        mergelist_hisat = rules.list_for_prepDE.output.hisat_list,
    output :
        gcsv_hisat = f'{out_dir}/3_count/STRINGTIE/HISAT_gene_count_matrix.csv',
        tcsv_hisat = f'{out_dir}/3_count/STRINGTIE/HISAT_transcript_count_matrix.csv',
    singularity:
        config["SINGULARITY"]["MAIN"]
    shell:
        """
        prepDE.py -i {input.mergelist_hisat} -t {output.tcsv_hisat} -g {output.gcsv_hisat}
        """

rule diff_exp_analysis:
    """
       Experimental sRNA Clusters Differential Expression Analysis
    """
    threads: get_threads('diff_exp_analysis',4)
    input:
        gcsv_hisat=rules.prepDE_stringtie_table.output.gcsv_hisat,
        sample_info=config["DATA"]["files"]["sample_info"],
        #de_comparisons_file=config["DATA"]["files"]["de_comparisons_file"],
        gtf_file=config["DATA"]["files"]["annotation"],
    params:
        out_dir = lambda w, output: os.path.dirname(output.dianeCounts),
        #normCount= f"{out_dir}/4_DE_analysis/filtered_normCounts.csv",
        #Counts=f"{out_dir}/4_DE_analysis/filtered_Counts.csv",
        #resDE= f"{out_dir}/4_DE_analysis/resDE.csv",
        #dianeCounts=f"{out_dir}/4_DE_analysis/diane_Counts.csv",
        #cutoff_cpm=config["DE"]["cutoff_cpm"],
        #cutoff_nb_echantillons=config["DE"]["cutoff_nb_echantillons"],
        #thres_FDR=config["DE"]["thres_FDR"],
        #thres_logFC=config["DE"]["thres_logFC"],
    output:
        dianeCounts=f"{out_dir}/4_DE_analysis/diane_Counts.csv",
    log:
        error = f"{log_dir}/diff_exp_analysis.e",
        output = f"{log_dir}/diff_exp_analysis.o"
    singularity:
        config["SINGULARITY"]["MAIN"]
    script:
        f"{RNAja.RNAJA_SCRIPTS}/EdgeR.R"
